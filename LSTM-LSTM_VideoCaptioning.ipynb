{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 34749,
     "status": "ok",
     "timestamp": 1673207875226,
     "user": {
      "displayName": "YASH NIKHARE",
      "userId": "17573027220439733493"
     },
     "user_tz": 300
    },
    "id": "FjRVi3tUHnSF",
    "outputId": "32d52a90-c1a0-48ae-d8ac-ce970d857413"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'training_data\\\\training_label.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 38\u001b[0m\n\u001b[0;32m     33\u001b[0m UNK_TOKEN \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<unk>\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# -------------------------------\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# 1. Load and preprocess captions from JSON\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# -------------------------------\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mTRAIN_LABEL_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     39\u001b[0m     data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Build a list of (caption, video_id) pairs.\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# For each caption, add <bos> and <eos> tokens and filter by length.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'training_data\\\\training_label.json'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import pickle\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# -------------------------------\n",
    "# Settings and Hyperparameters\n",
    "# -------------------------------\n",
    "\n",
    "FEAT_DIR = os.path.join(os.getcwd(), \"features\", \"feat_train\")\n",
    "train_split = 0.85\n",
    "max_length = 10               # maximum caption length (including <bos> and <eos>)\n",
    "latent_dim = 256              # LSTM hidden state dimension\n",
    "time_steps_encoder = 20       # number of frames (features) per video\n",
    "num_encoder_tokens = 2560     # dimension of each feature vector\n",
    "num_decoder_tokens = 1500     # maximum vocabulary size (including special tokens)\n",
    "batch_size = 100\n",
    "num_epochs = 30\n",
    "learning_rate = 0.0001\n",
    "\n",
    "# Special tokens\n",
    "PAD_TOKEN = \"<pad>\"\n",
    "BOS_TOKEN = \"<bos>\"\n",
    "EOS_TOKEN = \"<eos>\"\n",
    "UNK_TOKEN = \"<unk>\"\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Load and preprocess captions from JSON\n",
    "# -------------------------------\n",
    "with open(TRAIN_VIDEO_LIST, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Build a list of (caption, video_id) pairs.\n",
    "# For each caption, add <bos> and <eos> tokens and filter by length.\n",
    "train_list = []\n",
    "for item in data:\n",
    "    video_id = item[\"id\"]\n",
    "    if video_id.split(\".\")[0] not in valid_video_ids:\n",
    "        continue  # skip video not in training set\n",
    "    for caption in item[\"caption\"]:\n",
    "        caption_full = BOS_TOKEN + \" \" + caption + \" \" + EOS_TOKEN\n",
    "        if 6 <= len(caption_full.split()) <= 10:\n",
    "            train_list.append((caption_full, video_id))\n",
    "\n",
    "print(\"Total valid caption–video pairs:\", len(train_list))\n",
    "\n",
    "# Shuffle and split into training and validation sets\n",
    "random.shuffle(train_list)\n",
    "split_index = int(len(train_list) * train_split)\n",
    "training_list = train_list[:split_index]\n",
    "validation_list = train_list[split_index:]\n",
    "print(\"Training samples:\", len(training_list), \"Validation samples:\", len(validation_list))\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Build Vocabulary from training captions\n",
    "# -------------------------------\n",
    "# Count words from training captions\n",
    "counter = Counter()\n",
    "for caption, _ in training_list:\n",
    "    counter.update(caption.split())\n",
    "\n",
    "# Remove special tokens if they appear in the counter (we add them manually)\n",
    "for token in [PAD_TOKEN, BOS_TOKEN, EOS_TOKEN, UNK_TOKEN]:\n",
    "    if token in counter:\n",
    "        del counter[token]\n",
    "\n",
    "# Allow top (num_decoder_tokens - number_of_special_tokens) words.\n",
    "num_allowed = num_decoder_tokens - 4\n",
    "most_common = counter.most_common(num_allowed)\n",
    "vocab_words = [w for w, _ in most_common]\n",
    "\n",
    "# Build vocabulary: reserve index 0 for PAD, then BOS, EOS, UNK, then other words.\n",
    "vocab = {}\n",
    "vocab[PAD_TOKEN] = 0\n",
    "vocab[BOS_TOKEN] = 1\n",
    "vocab[EOS_TOKEN] = 2\n",
    "vocab[UNK_TOKEN] = 3\n",
    "index = 4\n",
    "for word in vocab_words:\n",
    "    vocab[word] = index\n",
    "    index += 1\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "print(\"Vocabulary size:\", vocab_size)\n",
    "\n",
    "def caption_to_seq(caption, vocab, max_length):\n",
    "    \"\"\"\n",
    "    Convert a caption string into a list of integer indices.\n",
    "    Pads or truncates the sequence to max_length.\n",
    "    \"\"\"\n",
    "    words = caption.split()\n",
    "    seq = [vocab.get(w, vocab[UNK_TOKEN]) for w in words]\n",
    "    if len(seq) < max_length:\n",
    "        seq += [vocab[PAD_TOKEN]] * (max_length - len(seq))\n",
    "    else:\n",
    "        seq = seq[:max_length]\n",
    "    return seq\n",
    "\n",
    "# Save the vocabulary for later use.\n",
    "with open(os.path.join(train_path, \"vocab.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(vocab, f)\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Load Pre-Extracted Features\n",
    "# -------------------------------\n",
    "# x_data maps a video ID (without extension) to its features (a numpy array).\n",
    "x_data = {}\n",
    "for filename in os.listdir(FEAT_DIR):\n",
    "    if filename.endswith(\".npy\"):\n",
    "        video_id = filename[:-4]\n",
    "        feat = np.load(os.path.join(FEAT_DIR, filename))\n",
    "        x_data[video_id] = feat\n",
    "print(\"Loaded features for\", len(x_data), \"videos.\")\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Create PyTorch Dataset and DataLoader\n",
    "# -------------------------------\n",
    "class VideoCaptionDataset(Dataset):\n",
    "    def __init__(self, caption_list, x_data, vocab, max_length):\n",
    "        self.caption_list = caption_list  # list of (caption, video_id)\n",
    "        self.x_data = x_data\n",
    "        self.vocab = vocab\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.caption_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        caption, video_id = self.caption_list[idx]\n",
    "        # Remove file extension if present\n",
    "        video_id = video_id.split(\".\")[0]\n",
    "        if video_id not in self.x_data:\n",
    "            raise ValueError(f\"Feature for video ID {video_id} not found.\")\n",
    "        encoder_feat = self.x_data[video_id]  # NumPy array of shape (time_steps_encoder, num_encoder_tokens)\n",
    "        encoder_feat = torch.tensor(encoder_feat, dtype=torch.float)\n",
    "\n",
    "        seq = caption_to_seq(caption, self.vocab, self.max_length)\n",
    "        seq = torch.tensor(seq, dtype=torch.long)\n",
    "        # For teacher forcing: decoder input is seq[:-1] and target is seq[1:].\n",
    "        decoder_input = seq[:-1]  # length = max_length - 1\n",
    "        decoder_target = seq[1:]\n",
    "        return encoder_feat, decoder_input, decoder_target\n",
    "\n",
    "train_dataset = VideoCaptionDataset(training_list, x_data, vocab, max_length)\n",
    "val_dataset = VideoCaptionDataset(validation_list, x_data, vocab, max_length)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "\n",
    "# -------------------------------\n",
    "# 5. Define the Encoder–Decoder (LSTM–LSTM) Model in PyTorch\n",
    "# -------------------------------\n",
    "class BiLSTMEncoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers=1):\n",
    "        super(BiLSTMEncoder, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        outputs, (h, c) = self.lstm(x)\n",
    "        # For a single-layer bidirectional LSTM:\n",
    "        # h has shape (2, batch, hidden_size) -> concatenate along hidden dimension\n",
    "        h_cat = torch.cat((h[0], h[1]), dim=-1)  # shape: (batch, 2*hidden_size)\n",
    "        c_cat = torch.cat((c[0], c[1]), dim=-1)\n",
    "        # Add a layer dimension for compatibility with decoder (assuming num_layers=1)\n",
    "        return h_cat.unsqueeze(0), c_cat.unsqueeze(0)\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers=1, dropout=0.5):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "    \n",
    "    def forward(self, x, hidden, cell):\n",
    "        # x: (batch, seq_len)\n",
    "        embedded = self.embedding(x)  # (batch, seq_len, embed_size)\n",
    "        outputs, (h, c) = self.lstm(embedded, (hidden, cell))\n",
    "        logits = self.fc(outputs)     # (batch, seq_len, vocab_size)\n",
    "        return logits, h, c\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "    \n",
    "    def forward(self, encoder_input, decoder_input):\n",
    "        # Encoder\n",
    "        hidden, cell = self.encoder(encoder_input)\n",
    "        # Decoder (using teacher forcing)\n",
    "        logits, _, _ = self.decoder(decoder_input, hidden, cell)\n",
    "        return logits\n",
    "\n",
    "# Hyperparameters for the model\n",
    "embed_size = 256\n",
    "encoder = BiLSTMEncoder(num_encoder_tokens, latent_dim)\n",
    "decoder = Decoder(vocab_size, embed_size, latent_dim**2)\n",
    "model = Seq2Seq(encoder, decoder)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(model)\n",
    "\n",
    "# -------------------------------\n",
    "# 6. Loss, Optimizer, and Training Loop\n",
    "# -------------------------------\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)  # ignore PAD token (index 0)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for encoder_input, decoder_input, decoder_target in dataloader:\n",
    "        encoder_input = encoder_input.to(device)   # (B, time_steps, num_encoder_tokens)\n",
    "        decoder_input = decoder_input.to(device)   # (B, max_length-1)\n",
    "        decoder_target = decoder_target.to(device) # (B, max_length-1)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(encoder_input, decoder_input)  # (B, seq_len, vocab_size)\n",
    "        # Reshape for loss computation\n",
    "        outputs = outputs.view(-1, vocab_size)\n",
    "        decoder_target = decoder_target.view(-1)\n",
    "        loss = criterion(outputs, decoder_target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss / len(dataloader)\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for encoder_input, decoder_input, decoder_target in dataloader:\n",
    "            encoder_input = encoder_input.to(device)\n",
    "            decoder_input = decoder_input.to(device)\n",
    "            decoder_target = decoder_target.to(device)\n",
    "            outputs = model(encoder_input, decoder_input)\n",
    "            outputs = outputs.view(-1, vocab_size)\n",
    "            decoder_target = decoder_target.view(-1)\n",
    "            loss = criterion(outputs, decoder_target)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "    val_loss = evaluate(model, val_loader, criterion, device)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}: Train Loss {train_loss:.4f}, Val Loss {val_loss:.4f}\")\n",
    "    # Save best model checkpoint\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), os.path.join(train_path, \"best_seq2seq_model.pth\"))\n",
    "\n",
    "# -------------------------------\n",
    "# 7. Save final model and vocabulary\n",
    "# -------------------------------\n",
    "# save_model_path = os.path.join(train_path, \"modelLSTM_LSTM\")\n",
    "save_model_path = os.path.join(os.getcwd(), \"model_final_2\")\n",
    "\n",
    "os.makedirs(save_model_path, exist_ok=True)\n",
    "torch.save(encoder.state_dict(), os.path.join(save_model_path, \"encoder_model_LSTM_LSTM.pth\"))\n",
    "torch.save(decoder.state_dict(), os.path.join(save_model_path, \"decoder_model_LSTM_LSTM.pth\"))\n",
    "with open(os.path.join(save_model_path, \"vocab.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(vocab, f)\n",
    "\n",
    "print(\"Training complete and models saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TyN9aeSCLUDM"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNR1mYvhHDJ9XpdvWXf7Miz",
   "name": "",
   "version": ""
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
